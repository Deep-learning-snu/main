{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13559cdb",
   "metadata": {},
   "source": [
    "## 참고 :  https://velog.io/@vector13/Food101-데이터셋을-이용한-음식-이미지-분류기-만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f35fd0a",
   "metadata": {},
   "source": [
    "### 1. 기본 환경 설정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a159e8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "/device:GPU:0\n",
      "Metal device set to: Apple M1\n"
     ]
    }
   ],
   "source": [
    "from shutil import copy\n",
    "from collections import defaultdict\n",
    "import scipy\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# 0 = all messages are logged (default behavior)\n",
    "# 1 = INFO messages are not printed\n",
    "# 2 = INFO and WARNING messages are not printed\n",
    "# 3 = INFO, WARNING, and ERROR messages are not printed\n",
    "\n",
    "# Theano(th)와 Tensorflow(tf) 모두와 호환이 되는 Keras 모듈을 작성\n",
    "import keras.backend as K\n",
    "from keras import regularizers\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator # 데이터 전처리\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0347c8",
   "metadata": {},
   "source": [
    "### 2. food101 다운 소스 : https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/\n",
    "* 주의 : food101 데이터 다운받은 후 train/test 나눌 때 한번만 코드 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bd0496b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Copying images into  apple_pie\n",
      "\n",
      "Copying images into  baby_back_ribs\n",
      "\n",
      "Copying images into  baklava\n",
      "\n",
      "Copying images into  beef_carpaccio\n",
      "\n",
      "Copying images into  beef_tartare\n",
      "\n",
      "Copying images into  beet_salad\n",
      "\n",
      "Copying images into  beignets\n",
      "\n",
      "Copying images into  bibimbap\n",
      "\n",
      "Copying images into  bread_pudding\n",
      "\n",
      "Copying images into  breakfast_burrito\n",
      "\n",
      "Copying images into  bruschetta\n",
      "\n",
      "Copying images into  caesar_salad\n",
      "\n",
      "Copying images into  cannoli\n",
      "\n",
      "Copying images into  caprese_salad\n",
      "\n",
      "Copying images into  carrot_cake\n",
      "\n",
      "Copying images into  ceviche\n",
      "\n",
      "Copying images into  cheesecake\n",
      "\n",
      "Copying images into  cheese_plate\n",
      "\n",
      "Copying images into  chicken_curry\n",
      "\n",
      "Copying images into  chicken_quesadilla\n",
      "\n",
      "Copying images into  chicken_wings\n",
      "\n",
      "Copying images into  chocolate_cake\n",
      "\n",
      "Copying images into  chocolate_mousse\n",
      "\n",
      "Copying images into  churros\n",
      "\n",
      "Copying images into  clam_chowder\n",
      "\n",
      "Copying images into  club_sandwich\n",
      "\n",
      "Copying images into  crab_cakes\n",
      "\n",
      "Copying images into  creme_brulee\n",
      "\n",
      "Copying images into  croque_madame\n",
      "\n",
      "Copying images into  cup_cakes\n",
      "\n",
      "Copying images into  deviled_eggs\n",
      "\n",
      "Copying images into  donuts\n",
      "\n",
      "Copying images into  dumplings\n",
      "\n",
      "Copying images into  edamame\n",
      "\n",
      "Copying images into  eggs_benedict\n",
      "\n",
      "Copying images into  escargots\n",
      "\n",
      "Copying images into  falafel\n",
      "\n",
      "Copying images into  filet_mignon\n",
      "\n",
      "Copying images into  fish_and_chips\n",
      "\n",
      "Copying images into  foie_gras\n",
      "\n",
      "Copying images into  french_fries\n",
      "\n",
      "Copying images into  french_onion_soup\n",
      "\n",
      "Copying images into  french_toast\n",
      "\n",
      "Copying images into  fried_calamari\n",
      "\n",
      "Copying images into  fried_rice\n",
      "\n",
      "Copying images into  frozen_yogurt\n",
      "\n",
      "Copying images into  garlic_bread\n",
      "\n",
      "Copying images into  gnocchi\n",
      "\n",
      "Copying images into  greek_salad\n",
      "\n",
      "Copying images into  grilled_cheese_sandwich\n",
      "\n",
      "Copying images into  grilled_salmon\n",
      "\n",
      "Copying images into  guacamole\n",
      "\n",
      "Copying images into  gyoza\n",
      "\n",
      "Copying images into  hamburger\n",
      "\n",
      "Copying images into  hot_and_sour_soup\n",
      "\n",
      "Copying images into  hot_dog\n",
      "\n",
      "Copying images into  huevos_rancheros\n",
      "\n",
      "Copying images into  hummus\n",
      "\n",
      "Copying images into  ice_cream\n",
      "\n",
      "Copying images into  lasagna\n",
      "\n",
      "Copying images into  lobster_bisque\n",
      "\n",
      "Copying images into  lobster_roll_sandwich\n",
      "\n",
      "Copying images into  macaroni_and_cheese\n",
      "\n",
      "Copying images into  macarons\n",
      "\n",
      "Copying images into  miso_soup\n",
      "\n",
      "Copying images into  mussels\n",
      "\n",
      "Copying images into  nachos\n",
      "\n",
      "Copying images into  omelette\n",
      "\n",
      "Copying images into  onion_rings\n",
      "\n",
      "Copying images into  oysters\n",
      "\n",
      "Copying images into  pad_thai\n",
      "\n",
      "Copying images into  paella\n",
      "\n",
      "Copying images into  pancakes\n",
      "\n",
      "Copying images into  panna_cotta\n",
      "\n",
      "Copying images into  peking_duck\n",
      "\n",
      "Copying images into  pho\n",
      "\n",
      "Copying images into  pizza\n",
      "\n",
      "Copying images into  pork_chop\n",
      "\n",
      "Copying images into  poutine\n",
      "\n",
      "Copying images into  prime_rib\n",
      "\n",
      "Copying images into  pulled_pork_sandwich\n",
      "\n",
      "Copying images into  ramen\n",
      "\n",
      "Copying images into  ravioli\n",
      "\n",
      "Copying images into  red_velvet_cake\n",
      "\n",
      "Copying images into  risotto\n",
      "\n",
      "Copying images into  samosa\n",
      "\n",
      "Copying images into  sashimi\n",
      "\n",
      "Copying images into  scallops\n",
      "\n",
      "Copying images into  seaweed_salad\n",
      "\n",
      "Copying images into  shrimp_and_grits\n",
      "\n",
      "Copying images into  spaghetti_bolognese\n",
      "\n",
      "Copying images into  spaghetti_carbonara\n",
      "\n",
      "Copying images into  spring_rolls\n",
      "\n",
      "Copying images into  steak\n",
      "\n",
      "Copying images into  strawberry_shortcake\n",
      "\n",
      "Copying images into  sushi\n",
      "\n",
      "Copying images into  tacos\n",
      "\n",
      "Copying images into  takoyaki\n",
      "\n",
      "Copying images into  tiramisu\n",
      "\n",
      "Copying images into  tuna_tartare\n",
      "\n",
      "Copying images into  waffles\n",
      "Copying Done!\n",
      "\n",
      "Copying images into  apple_pie\n",
      "\n",
      "Copying images into  baby_back_ribs\n",
      "\n",
      "Copying images into  baklava\n",
      "\n",
      "Copying images into  beef_carpaccio\n",
      "\n",
      "Copying images into  beef_tartare\n",
      "\n",
      "Copying images into  beet_salad\n",
      "\n",
      "Copying images into  beignets\n",
      "\n",
      "Copying images into  bibimbap\n",
      "\n",
      "Copying images into  bread_pudding\n",
      "\n",
      "Copying images into  breakfast_burrito\n",
      "\n",
      "Copying images into  bruschetta\n",
      "\n",
      "Copying images into  caesar_salad\n",
      "\n",
      "Copying images into  cannoli\n",
      "\n",
      "Copying images into  caprese_salad\n",
      "\n",
      "Copying images into  carrot_cake\n",
      "\n",
      "Copying images into  ceviche\n",
      "\n",
      "Copying images into  cheesecake\n",
      "\n",
      "Copying images into  cheese_plate\n",
      "\n",
      "Copying images into  chicken_curry\n",
      "\n",
      "Copying images into  chicken_quesadilla\n",
      "\n",
      "Copying images into  chicken_wings\n",
      "\n",
      "Copying images into  chocolate_cake\n",
      "\n",
      "Copying images into  chocolate_mousse\n",
      "\n",
      "Copying images into  churros\n",
      "\n",
      "Copying images into  clam_chowder\n",
      "\n",
      "Copying images into  club_sandwich\n",
      "\n",
      "Copying images into  crab_cakes\n",
      "\n",
      "Copying images into  creme_brulee\n",
      "\n",
      "Copying images into  croque_madame\n",
      "\n",
      "Copying images into  cup_cakes\n",
      "\n",
      "Copying images into  deviled_eggs\n",
      "\n",
      "Copying images into  donuts\n",
      "\n",
      "Copying images into  dumplings\n",
      "\n",
      "Copying images into  edamame\n",
      "\n",
      "Copying images into  eggs_benedict\n",
      "\n",
      "Copying images into  escargots\n",
      "\n",
      "Copying images into  falafel\n",
      "\n",
      "Copying images into  filet_mignon\n",
      "\n",
      "Copying images into  fish_and_chips\n",
      "\n",
      "Copying images into  foie_gras\n",
      "\n",
      "Copying images into  french_fries\n",
      "\n",
      "Copying images into  french_onion_soup\n",
      "\n",
      "Copying images into  french_toast\n",
      "\n",
      "Copying images into  fried_calamari\n",
      "\n",
      "Copying images into  fried_rice\n",
      "\n",
      "Copying images into  frozen_yogurt\n",
      "\n",
      "Copying images into  garlic_bread\n",
      "\n",
      "Copying images into  gnocchi\n",
      "\n",
      "Copying images into  greek_salad\n",
      "\n",
      "Copying images into  grilled_cheese_sandwich\n",
      "\n",
      "Copying images into  grilled_salmon\n",
      "\n",
      "Copying images into  guacamole\n",
      "\n",
      "Copying images into  gyoza\n",
      "\n",
      "Copying images into  hamburger\n",
      "\n",
      "Copying images into  hot_and_sour_soup\n",
      "\n",
      "Copying images into  hot_dog\n",
      "\n",
      "Copying images into  huevos_rancheros\n",
      "\n",
      "Copying images into  hummus\n",
      "\n",
      "Copying images into  ice_cream\n",
      "\n",
      "Copying images into  lasagna\n",
      "\n",
      "Copying images into  lobster_bisque\n",
      "\n",
      "Copying images into  lobster_roll_sandwich\n",
      "\n",
      "Copying images into  macaroni_and_cheese\n",
      "\n",
      "Copying images into  macarons\n",
      "\n",
      "Copying images into  miso_soup\n",
      "\n",
      "Copying images into  mussels\n",
      "\n",
      "Copying images into  nachos\n",
      "\n",
      "Copying images into  omelette\n",
      "\n",
      "Copying images into  onion_rings\n",
      "\n",
      "Copying images into  oysters\n",
      "\n",
      "Copying images into  pad_thai\n",
      "\n",
      "Copying images into  paella\n",
      "\n",
      "Copying images into  pancakes\n",
      "\n",
      "Copying images into  panna_cotta\n",
      "\n",
      "Copying images into  peking_duck\n",
      "\n",
      "Copying images into  pho\n",
      "\n",
      "Copying images into  pizza\n",
      "\n",
      "Copying images into  pork_chop\n",
      "\n",
      "Copying images into  poutine\n",
      "\n",
      "Copying images into  prime_rib\n",
      "\n",
      "Copying images into  pulled_pork_sandwich\n",
      "\n",
      "Copying images into  ramen\n",
      "\n",
      "Copying images into  ravioli\n",
      "\n",
      "Copying images into  red_velvet_cake\n",
      "\n",
      "Copying images into  risotto\n",
      "\n",
      "Copying images into  samosa\n",
      "\n",
      "Copying images into  sashimi\n",
      "\n",
      "Copying images into  scallops\n",
      "\n",
      "Copying images into  seaweed_salad\n",
      "\n",
      "Copying images into  shrimp_and_grits\n",
      "\n",
      "Copying images into  spaghetti_bolognese\n",
      "\n",
      "Copying images into  spaghetti_carbonara\n",
      "\n",
      "Copying images into  spring_rolls\n",
      "\n",
      "Copying images into  steak\n",
      "\n",
      "Copying images into  strawberry_shortcake\n",
      "\n",
      "Copying images into  sushi\n",
      "\n",
      "Copying images into  tacos\n",
      "\n",
      "Copying images into  takoyaki\n",
      "\n",
      "Copying images into  tiramisu\n",
      "\n",
      "Copying images into  tuna_tartare\n",
      "\n",
      "Copying images into  waffles\n",
      "Copying Done!\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(filepath, src,dest):\n",
    "  classes_images = defaultdict(list)\n",
    "  with open(filepath, 'r') as txt:\n",
    "      paths = [read.strip() for read in txt.readlines()]\n",
    "      for p in paths:\n",
    "        food = p.split('/')\n",
    "        classes_images[food[0]].append(food[1] + '.jpg')\n",
    "\n",
    "  for food in classes_images.keys():\n",
    "    print(\"\\nCopying images into \",food)\n",
    "    if not os.path.exists(os.path.join(dest,food)):\n",
    "      os.makedirs(os.path.join(dest,food))\n",
    "    for i in classes_images[food]:\n",
    "      copy(os.path.join(src,food,i), os.path.join(dest,food,i))\n",
    "  print(\"Copying Done!\")\n",
    "\n",
    "# 본인 환경에 맞게 food101 들어있는 디렉토리 설정하기\n",
    "prepare_data('/Volumes/T7/food-101/meta/train.txt', '/Volumes/T7/food-101/images', '/Volumes/T7/food-101/train')\n",
    "prepare_data('/Volumes/T7/food-101/meta/test.txt', '/Volumes/T7/food-101/images', '/Volumes/T7/food-101/test')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b651dc",
   "metadata": {},
   "source": [
    "### 3. food101 데이터 기본 세팅 (전처리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3964236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# releases the global state: avoid clutter from old models and layers\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4603d682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기 변수 설정\n",
    "n_classes = 101\n",
    "img_width, img_height = 299, 299\n",
    "train_data_dir = '/Volumes/T7/food-101/train'\n",
    "validation_data_dir = '/Volumes/T7/food-101/test'\n",
    "nb_train_samples = 75750       # 750 * 101\n",
    "nb_validation_samples = 25250  # 250 * 101\n",
    "batch_size = 20                # 한 번 실행될 때 생성할 이미지 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f14c6d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageDataGenerator 객체 생성 (이미지 파일들을 Numpy Array 형태로 가져온 후 증강 기법 적용 준비)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,       # multiply the data by the value\n",
    "    shear_range=0.2,        # Shear angle in counter-clockwise direction as radians\n",
    "    zoom_range=0.2,         # Range for random zoom. If a float\n",
    "    horizontal_flip=True)   # Randomly flip inputs horizontally\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eb911e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75750 images belonging to 101 classes.\n",
      "Found 25250 images belonging to 101 classes.\n"
     ]
    }
   ],
   "source": [
    "# flow_from_directory : meta data가 담긴 디렉토리 설정\n",
    "# meta data : 이미지 데이터 파일들과 해당 이미지들이 무슨 이미지를 나타내는지 텍스트로 표현한 문자열\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ebffd6",
   "metadata": {},
   "source": [
    "### 4. 학습 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e80c695d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "# MobileNetV2 모델 생성\n",
    "\n",
    "mbv2 = MobileNetV2(weights = 'imagenet', include_top = False, input_shape = (299,299,3))\n",
    "# weights : None (random initialization), 'imagenet' (pre-training on ImageNet), or the path to the weights file to be loaded.\n",
    "# include_top : include the fully-connected layer at the top of the network\n",
    "x = mbv2.output\n",
    "x = GlobalAveragePooling2D()(x)     # pooling layer\n",
    "x = Dense(128,activation='relu')(x) # hidden layer : model can learn more complex functions and classify for better results.\n",
    "x = Dropout(0.2)(x)                 # 과적합을 방지하기 위해 무작위로 특정 노드(입력값)를 0으로 만든다. (여기서는 20%)\n",
    "predictions = Dense(101,kernel_regularizer = regularizers.l2(0.005), activation = 'softmax')(x) #  #final layer : with softmax activation for 101 classes\n",
    "\n",
    "model = Model(inputs = mbv2.input, outputs = predictions)\n",
    "model.compile(optimizer=SGD(learning_rate = 0.0001, momentum = 0.9), loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb86080",
   "metadata": {},
   "source": [
    "### 5. 모델 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d719eb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cc/6xhsqlks6mg1kzb03g_1zn7r0000gn/T/ipykernel_14141/911124647.py:5: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(train_generator,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 787/3787 [=====>........................] - ETA: 25:30 - loss: 5.1840 - accuracy: 0.0163"
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='best_model_3class_sept.hdf5', verbose=1, save_best_only=True)\n",
    "csv_logger = CSVLogger('history.log')\n",
    "\n",
    "# 주로 fit() 함수를 사용하지만 제네레이터로 생성된 배치로 학습시킬 경우에는 fit_generator() 함수를 사용\n",
    "history = model.fit_generator(train_generator,\n",
    "                    steps_per_epoch = nb_train_samples // batch_size,      #  한 epoch에 사용한 스텝 수\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=nb_validation_samples // batch_size,  # 한 epoch 종료 시 마다 검증할 때 사용되는 검증 스텝 수\n",
    "                    epochs=10,                                             # 전체 훈련 데이터셋에 대해 학습 반복 횟수\n",
    "                    verbose=1,\n",
    "                    callbacks=[csv_logger, checkpointer])\n",
    "\n",
    "model.save('model_trained.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81bebbd",
   "metadata": {},
   "source": [
    "### 6. 모델 예측하기 (predict_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17984bc",
   "metadata": {},
   "source": [
    "### 7. 모델 사용하기 (predict_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52761a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import utils\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace5e975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a list of all the foods, in the argument i put the path to the folder that has all folders for food\n",
    "def create_foodlist(path):\n",
    "    list_ = list()\n",
    "    for root, dirs, files in os.walk(path, topdown=False):\n",
    "      for name in dirs:\n",
    "        list_.append(name)\n",
    "    return list_    \n",
    "\n",
    "#loading the model i trained and finetuned        \n",
    "my_model = load_model('model_trained.h5', compile = False)\n",
    "food_list = create_foodlist(\"food101/images\")\n",
    "\n",
    "#function to help in predicting classes of new images loaded from my computer(for now) \n",
    "def predict_class(model, images, show = True):\n",
    "  for img in images:\n",
    "    img = utils.load_img(img, grayscale=False, color_mode='rgb', target_size=(299, 299))\n",
    "\n",
    "    img = utils.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)         \n",
    "    img /= 255.                                      \n",
    "\n",
    "    pred = model.predict(img)\n",
    "    index = np.argmax(pred)    #Returns the indices of the maximum values along an axis, In case of multiple occurrences of the maximum values, the indices corresponding to the first occurrence are returned.\n",
    "    food_list.sort()\n",
    "    pred_value = food_list[index]\n",
    "    if show:\n",
    "        plt.imshow(img[0])                           \n",
    "        plt.axis('off')\n",
    "        plt.title(pred_value)\n",
    "        plt.show()\n",
    "\n",
    "#add the images you want to predict into a list (these are in the WD)\n",
    "images = []\n",
    "images.append('sc.jpg')\n",
    "images.append('vb.jpg')\n",
    "images.append('vp.jpg')\n",
    "images.append('es.jpg')\n",
    "\n",
    "\n",
    "print(\"PREDICTIONS BASED ON PICTURES UPLOADED\")\n",
    "predict_class(my_model, images, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
