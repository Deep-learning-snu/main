{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7552ba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copy\n",
    "from collections import defaultdict\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import cv2\n",
    "import glob\n",
    "from PIL import Image, ImageEnhance\n",
    "import PIL.ImageOps\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# 0 = all messages are logged (default behavior)\n",
    "# 1 = INFO messages are not printed\n",
    "# 2 = INFO and WARNING messages are not printed\n",
    "# 3 = INFO, WARNING, and ERROR messages are not printed\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Theano(th)와 Tensorflow(tf) 모두와 호환이 되는 Keras 모듈을 작성\n",
    "import keras.backend as K\n",
    "from keras import regularizers\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Conv2D\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator # 데이터 전처리\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "#from keras.optimizers import SGD\n",
    "\n",
    "from keras_preprocessing.image import array_to_img, img_to_array, load_img\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from deeplab2.model.pixel_encoder import moat\n",
    "from AdaBelief_tf import AdaBeliefOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b63e9a",
   "metadata": {},
   "source": [
    "## Data augmentation (한번만 실행)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "071d244e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 train done!\n"
     ]
    }
   ],
   "source": [
    "file_train_path = \"/Users/dayeonjung/Desktop/df/catdog/train/\"\n",
    "\n",
    "catdog_list = os.listdir(file_train_path)\n",
    "\n",
    "catdog_list.remove('.DS_Store')\n",
    "\n",
    "catdog_list = list(map(int, catdog_list))\n",
    "catdog_list.sort()\n",
    "catdog_list = list(map(str, catdog_list))\n",
    "\n",
    "for i in range(len(catdog_list)):\n",
    "    file_path = file_train_path + catdog_list[i] + '/'\n",
    "    x = os.listdir(file_path)\n",
    "    file_names=[]\n",
    "    for fn in x:\n",
    "        if '.jpg' not in fn:\n",
    "            continue\n",
    "        else:\n",
    "            file_names.append(fn)\n",
    "\n",
    "    for j in range(0, len(file_names)):\n",
    "        file_name = file_names[j]\n",
    "        origin_image_path = file_path +  file_name\n",
    "        x = Image.open(origin_image_path)\n",
    "\n",
    "        x = img_to_array(x)\n",
    "        x = x/255 \n",
    "        \n",
    "\n",
    "        if x.shape[2]==3:\n",
    "            grayscaled = tf.image.rgb_to_grayscale(x)\n",
    "            saturated = tf.image.adjust_saturation(x, 3)\n",
    "            bright = ImageEnhance.Brightness(array_to_img(x)).enhance(2.0)\n",
    "        elif x.shape[2]>3:\n",
    "            grayscaled = tf.image.rgb_to_grayscale(x[:,:,:3])\n",
    "            saturated = tf.image.adjust_saturation(x[:,:,:3], 3)\n",
    "            bright = ImageEnhance.Brightness(array_to_img(x[:,:,:3])).enhance(2.0)\n",
    "        else:\n",
    "            grayscaled = x\n",
    "            saturated = x\n",
    "            bright = x\n",
    "\n",
    "        array_to_img(grayscaled).save(file_path + 'gray_' + file_name)\n",
    "        array_to_img(saturated).save(file_path + 'saturated_' + file_name)\n",
    "        array_to_img(bright).save(file_path + 'bright_' + file_name)\n",
    "    print(i+1, end = ' ')\n",
    "\n",
    "print(\"train done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "867efb59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# releases the global state: avoid clutter from old models and layers\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4355bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기 변수 설정\n",
    "n_classes = 37\n",
    "\n",
    "### 이미지 크기 설정\n",
    "img_width, img_height = 256,256\n",
    "\n",
    "\n",
    "train_data_dir = '/Users/dayeonjung/Desktop/df/catdog/train'\n",
    "validation_data_dir = '/Users/dayeonjung/Desktop/df/catdog/validation/'\n",
    "nb_train_samples = 3312*4   # augmentation했으니 4 곱해주기     \n",
    "nb_validation_samples = 368  \n",
    "batch_size = 10   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85cc8cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageDataGenerator 객체 생성 (이미지 파일들을 Numpy Array 형태로 가져온 후 증강 기법 적용 준비)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./ 255,       # multiply the data by the value\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,        # Shear angle in counter-clockwise direction as radians\n",
    "    zoom_range=0.2,         # Range for random zoom. If a float\n",
    "    horizontal_flip=True,   # Randomly flip inputs horizontally\n",
    "    fill_mode='nearest')   \n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./ 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09d2ace5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13248 images belonging to 37 classes.\n",
      "Found 368 images belonging to 37 classes.\n"
     ]
    }
   ],
   "source": [
    "# flow_from_directory : Numpy Array Iterator 객체 생성\n",
    "# 인자로 설정해주는 directory의 바로 하위 디렉토리 이름을 레이블이라고 간주, 그 디렉토리 아래의 파일들을 해당 레이블의 이미지들이라고 알아서 추측\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='sparse')\n",
    "    #'categorical' : 멀티-레이블 클래스, 원-핫 인코딩된 형태\n",
    "    #'sparse' : 멀티-레이블 클래스, 레이블 인코딩된 형태\n",
    "    #'binary' : 이진 분류 클래스, 0 또는 1인 형태\n",
    "    \n",
    "    \n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='sparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "352b4ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    }
   ],
   "source": [
    "moat1=moat.get_model(\n",
    "    name='tiny_moat0_pretrain_256_no_pe_1k',input_shape=[256,256,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f958e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "moat1 = moat._load_moat_pretrained_checkpoint(moat1, path=\"./model-ckpt-0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eefce982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stage1': (None, 128, 128, 32),\n",
       " 'res1': (None, 128, 128, 32),\n",
       " 'stage2': (None, 64, 64, 32),\n",
       " 'res2': (None, 64, 64, 32),\n",
       " 'stage3': (None, 32, 32, 64),\n",
       " 'res3': (None, 32, 32, 64),\n",
       " 'stage4': (None, 16, 16, 128),\n",
       " 'res4': (None, 16, 16, 128),\n",
       " 'stage5': (None, 8, 8, 256),\n",
       " 'res5': (None, 8, 8, 256)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import (Input, Dense, concatenate, Conv2D, MaxPooling2D, Flatten)\n",
    "from tensorflow import keras\n",
    "\n",
    "moat1.input_shape\n",
    "moat1.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e370f018",
   "metadata": {},
   "outputs": [],
   "source": [
    "y11 = moat1.output[\"stage1\"]\n",
    "y12 = moat1.output[\"res1\"]\n",
    "y21 = moat1.output[\"stage2\"]\n",
    "y22 = moat1.output[\"res2\"]\n",
    "y31 = moat1.output[\"stage3\"]\n",
    "y32 = moat1.output[\"res3\"]\n",
    "y41 = moat1.output[\"stage4\"]\n",
    "y42 = moat1.output[\"res4\"]\n",
    "y51 = moat1.output[\"stage5\"]\n",
    "y52 = moat1.output[\"res5\"]\n",
    "\n",
    "z1 = y11+y12\n",
    "z2 = y21+y22\n",
    "z3 = y31+y32\n",
    "z4 = y41+y42\n",
    "z5 = y51+y52\n",
    "\n",
    "a1=Conv2D(8,(1,1),activation='relu')(z1)\n",
    "a2=Conv2D(8,(3,3),activation='relu')(a1)\n",
    "a3=MaxPooling2D(2,2)(a2) \n",
    "a4=Conv2D(8,(3,3),activation='relu')(a3)\n",
    "a5=MaxPooling2D(2,2)(a4) \n",
    "a6=Conv2D(8,(3,3),activation='relu')(a5)\n",
    "a7=MaxPooling2D(2,2)(a6) \n",
    "a8=Conv2D(8,(3,3),activation='relu')(a7)\n",
    "a9=MaxPooling2D(2,2)(a8)\n",
    "a10=Conv2D(8,(3,3),activation='relu')(a9)\n",
    "a11=MaxPooling2D(2,2)(a10)\n",
    "a12=Conv2D(32,(1,1),activation='relu')(a11)\n",
    "aa1=Flatten(name='flatten_layer1')(a12)\n",
    "\n",
    "b1=Conv2D(8,(1,1),activation='relu')(z2)\n",
    "b2=Conv2D(8,(3,3),activation='relu')(b1)\n",
    "b3=MaxPooling2D(2,2)(b2) \n",
    "b4=Conv2D(8,(3,3),activation='relu')(b3)\n",
    "b5=MaxPooling2D(2,2)(b4) \n",
    "b6=Conv2D(8,(3,3),activation='relu')(b5)\n",
    "b7=MaxPooling2D(2,2)(b6) \n",
    "b8=Conv2D(8,(3,3),activation='relu')(b7)\n",
    "b9=MaxPooling2D(2,2)(b8)\n",
    "b10=Conv2D(32,(1,1),activation='relu')(b9)\n",
    "bb1=Flatten(name='flatten_layer2')(b10)\n",
    "\n",
    "c1=Conv2D(16,(1,1),activation='relu')(z3)\n",
    "c2=Conv2D(16,(3,3),activation='relu')(c1)\n",
    "c3=MaxPooling2D(2,2)(c2) \n",
    "c4=Conv2D(16,(3,3),activation='relu')(c3)\n",
    "c5=MaxPooling2D(2,2)(c4) \n",
    "c6=Conv2D(16,(3,3),activation='relu')(c5)\n",
    "c7=MaxPooling2D(2,2)(c6) \n",
    "c8=Conv2D(64,(1,1),activation='relu')(c7)\n",
    "cc1=Flatten(name='flatten_layer3')(c8)\n",
    "\n",
    "d1=Conv2D(32,(1,1),activation='relu')(z4)\n",
    "d2=Conv2D(32,(3,3),activation='relu')(d1)\n",
    "d3=MaxPooling2D(2,2)(d2) \n",
    "d4=Conv2D(32,(3,3),activation='relu')(d3)\n",
    "d5=MaxPooling2D(2,2)(d4) \n",
    "d6=Conv2D(128,(1,1),activation='relu')(d5)\n",
    "dd1=Flatten(name='flatten_layer4')(d6)\n",
    "\n",
    "e1=Conv2D(64,(1,1),activation='relu')(z5)\n",
    "e2=Conv2D(64,(3,3),activation='relu')(e1)\n",
    "e3=MaxPooling2D(2,2)(e2) \n",
    "e4=Conv2D(64,(2,2),activation='relu')(e3)\n",
    "e5=Conv2D(256,(1,1),activation='relu')(e4)\n",
    "ee1=Flatten(name='flatten_layer5')(e5)\n",
    "\n",
    "fc1=tf.concat([aa1,bb1,cc1,dd1,ee1],axis=1)\n",
    "out1=Dense(units=37, activation='softmax')(fc1)\n",
    "\n",
    "model1 = tf.keras.models.Model(moat1.input, out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d5f9e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      ">=0.1.0 (Current 0.2.1)  1e-14  supported          default: True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended epsilon = 1e-7                                  Recommended epsilon = 1e-14\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# model1.load_weights('model1check.hdf5')\n",
    "\n",
    "opt = AdaBeliefOptimizer(learning_rate=1e-3, beta_1= 0.9, beta_2=0.999, weight_decay= 1e-4,  epsilon=5e-13, rectify=True)\n",
    "model1.compile(optimizer=opt,\n",
    "              loss='sparse_categorical_crossentropy', metrics = tf.keras.metrics.SparseCategoricalAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc37e95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cc/6xhsqlks6mg1kzb03g_1zn7r0000gn/T/ipykernel_8790/1685723023.py:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model1.fit_generator(train_generator,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  18/1324 [..............................] - ETA: 2:51:58 - loss: 4.0763 - sparse_categorical_accuracy: 0.0222"
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='best_model_3class_sept.hdf5', verbose=1, save_best_only=True)\n",
    "csv_logger = CSVLogger('history.log')\n",
    "\n",
    "history = model1.fit_generator(train_generator,\n",
    "                    steps_per_epoch = nb_train_samples // batch_size,      #  한 epoch에 사용한 스텝 수\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=nb_validation_samples // batch_size,  # 한 epoch 종료 시 마다 검증할 때 사용되는 검증 스텝 수\n",
    "                    epochs=50,                                             # 전체 훈련 데이터셋에 대해 학습 반복 횟수\n",
    "                    verbose=1,\n",
    "                    callbacks=[csv_logger, checkpointer])\n",
    "\n",
    "model1.save('model1_trained.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbc71a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e782a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c075d0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
