{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7552ba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copy\n",
    "from collections import defaultdict\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import cv2\n",
    "import glob\n",
    "from PIL import Image, ImageEnhance\n",
    "import PIL.ImageOps\n",
    "\n",
    "#import torch\n",
    "#import torchvision\n",
    "#from torch import nn\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# 0 = all messages are logged (default behavior)\n",
    "# 1 = INFO messages are not printed\n",
    "# 2 = INFO and WARNING messages are not printed\n",
    "# 3 = INFO, WARNING, and ERROR messages are not printed\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Theano(th)와 Tensorflow(tf) 모두와 호환이 되는 Keras 모듈을 작성\n",
    "import keras.backend as K\n",
    "from keras import regularizers\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Conv2D\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator # 데이터 전처리\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "#from keras.optimizers import SGD\n",
    "\n",
    "from keras_preprocessing.image import array_to_img, img_to_array, load_img\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from deeplab2.model.pixel_encoder import moat\n",
    "from AdaBelief_tf import AdaBeliefOptimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b63e9a",
   "metadata": {},
   "source": [
    "## Data augmentation (한번만 실행)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c5ef877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 train done!\n"
     ]
    }
   ],
   "source": [
    "file_train_path = \"/Volumes/T7/catdog/train/\"\n",
    "\n",
    "catdog_list = os.listdir(file_train_path)\n",
    "catdog_list = [x for x in catdog_list if 'DS' not in x]\n",
    "catdog_list = list(map(int, catdog_list))\n",
    "catdog_list.sort()\n",
    "catdog_list = list(map(str, catdog_list))\n",
    "catdog_list\n",
    "\n",
    "for i in range(len(catdog_list)):\n",
    "    file_path = file_train_path + catdog_list[i] + '/'\n",
    "    x = os.listdir(file_path)\n",
    "    file_names=[]\n",
    "    for fn in x:\n",
    "        if '.jpg' not in fn:\n",
    "            continue\n",
    "        else:\n",
    "            file_names.append(fn)\n",
    "\n",
    "    for j in range(0, len(file_names)):\n",
    "        file_name = file_names[j]\n",
    "        origin_image_path = file_path +  file_name\n",
    "        x = Image.open(origin_image_path)\n",
    "\n",
    "        x = img_to_array(x)\n",
    "        x = x/255 \n",
    "        \n",
    "\n",
    "        if x.shape[2]==3:\n",
    "            grayscaled = tf.image.rgb_to_grayscale(x)\n",
    "            saturated = tf.image.adjust_saturation(x, 3)\n",
    "            bright = ImageEnhance.Brightness(array_to_img(x)).enhance(2.0)\n",
    "        elif x.shape[2]>3:\n",
    "            grayscaled = tf.image.rgb_to_grayscale(x[:,:,:3])\n",
    "            saturated = tf.image.adjust_saturation(x[:,:,:3], 3)\n",
    "            bright = ImageEnhance.Brightness(array_to_img(x[:,:,:3])).enhance(2.0)\n",
    "        else:\n",
    "            grayscaled = x\n",
    "            saturated = x\n",
    "            bright = x\n",
    "\n",
    "        array_to_img(grayscaled).save(file_path + 'gray_' + file_name)\n",
    "        array_to_img(saturated).save(file_path + 'saturated_' + file_name)\n",
    "        array_to_img(bright).save(file_path + 'bright_' + file_name)\n",
    "    print(i+1, end = ' ')\n",
    "\n",
    "print(\"train done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "867efb59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# releases the global state: avoid clutter from old models and layers\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4355bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기 변수 설정\n",
    "n_classes = 37\n",
    "\n",
    "### 이미지 크기 설정\n",
    "img_width, img_height = 256,256\n",
    "\n",
    "\n",
    "train_data_dir = '/Volumes/T7/catdog/train/'\n",
    "validation_data_dir = '/Volumes/T7/catdog/validation/'\n",
    "nb_train_samples = 3312*4   # augmentation했으니 4 곱해주기     \n",
    "nb_validation_samples = 368  \n",
    "batch_size = 10   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85cc8cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageDataGenerator 객체 생성 (이미지 파일들을 Numpy Array 형태로 가져온 후 증강 기법 적용 준비)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./ 255,       # multiply the data by the value\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,        # Shear angle in counter-clockwise direction as radians\n",
    "    zoom_range=0.2,         # Range for random zoom. If a float\n",
    "    horizontal_flip=True,   # Randomly flip inputs horizontally\n",
    "    fill_mode='nearest')   \n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./ 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17d8582c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13248 images belonging to 37 classes.\n",
      "Found 368 images belonging to 37 classes.\n"
     ]
    }
   ],
   "source": [
    "# flow_from_directory : Numpy Array Iterator 객체 생성\n",
    "# 인자로 설정해주는 directory의 바로 하위 디렉토리 이름을 레이블이라고 간주, 그 디렉토리 아래의 파일들을 해당 레이블의 이미지들이라고 알아서 추측\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='sparse')\n",
    "    #'categorical' : 멀티-레이블 클래스, 원-핫 인코딩된 형태\n",
    "    #'sparse' : 멀티-레이블 클래스, 레이블 인코딩된 형태\n",
    "    #'binary' : 이진 분류 클래스, 0 또는 1인 형태\n",
    "    \n",
    "    \n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='sparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "352b4ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/hdmtlab/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "moat1=moat.get_model(\n",
    "    name='tiny_moat0_pretrain_256_no_pe_1k',input_shape=[256,256,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f958e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "moat1 = moat._load_moat_pretrained_checkpoint(moat1, path=\"./model-ckpt-0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eefce982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stage1': (None, 128, 128, 32),\n",
       " 'res1': (None, 128, 128, 32),\n",
       " 'stage2': (None, 64, 64, 32),\n",
       " 'res2': (None, 64, 64, 32),\n",
       " 'stage3': (None, 32, 32, 64),\n",
       " 'res3': (None, 32, 32, 64),\n",
       " 'stage4': (None, 16, 16, 128),\n",
       " 'res4': (None, 16, 16, 128),\n",
       " 'stage5': (None, 8, 8, 256),\n",
       " 'res5': (None, 8, 8, 256)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import (Input, Dense, concatenate, Conv2D, MaxPooling2D, Flatten)\n",
    "from tensorflow import keras\n",
    "\n",
    "moat1.input_shape\n",
    "moat1.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e370f018",
   "metadata": {},
   "outputs": [],
   "source": [
    "y11 = moat1.output[\"stage1\"]\n",
    "y12 = moat1.output[\"res1\"]\n",
    "y21 = moat1.output[\"stage2\"]\n",
    "y22 = moat1.output[\"res2\"]\n",
    "y31 = moat1.output[\"stage3\"]\n",
    "y32 = moat1.output[\"res3\"]\n",
    "y41 = moat1.output[\"stage4\"]\n",
    "y42 = moat1.output[\"res4\"]\n",
    "y51 = moat1.output[\"stage5\"]\n",
    "y52 = moat1.output[\"res5\"]\n",
    "\n",
    "z1 = y11+y12\n",
    "z2 = y21+y22\n",
    "z3 = y31+y32\n",
    "z4 = y41+y42\n",
    "z5 = y51+y52\n",
    "\n",
    "a1=Conv2D(8,(1,1),activation='relu')(z1)\n",
    "a2=Conv2D(8,(3,3),activation='relu')(a1)\n",
    "a3=MaxPooling2D(2,2)(a2) \n",
    "a4=Conv2D(8,(3,3),activation='relu')(a3)\n",
    "a5=MaxPooling2D(2,2)(a4) \n",
    "a6=Conv2D(8,(3,3),activation='relu')(a5)\n",
    "a7=MaxPooling2D(2,2)(a6) \n",
    "a8=Conv2D(8,(3,3),activation='relu')(a7)\n",
    "a9=MaxPooling2D(2,2)(a8)\n",
    "a10=Conv2D(8,(3,3),activation='relu')(a9)\n",
    "a11=MaxPooling2D(2,2)(a10)\n",
    "a12=Conv2D(32,(1,1),activation='relu')(a11)\n",
    "aa1=Flatten(name='flatten_layer1')(a12)\n",
    "\n",
    "b1=Conv2D(8,(1,1),activation='relu')(z2)\n",
    "b2=Conv2D(8,(3,3),activation='relu')(b1)\n",
    "b3=MaxPooling2D(2,2)(b2) \n",
    "b4=Conv2D(8,(3,3),activation='relu')(b3)\n",
    "b5=MaxPooling2D(2,2)(b4) \n",
    "b6=Conv2D(8,(3,3),activation='relu')(b5)\n",
    "b7=MaxPooling2D(2,2)(b6) \n",
    "b8=Conv2D(8,(3,3),activation='relu')(b7)\n",
    "b9=MaxPooling2D(2,2)(b8)\n",
    "b10=Conv2D(32,(1,1),activation='relu')(b9)\n",
    "bb1=Flatten(name='flatten_layer2')(b10)\n",
    "\n",
    "c1=Conv2D(16,(1,1),activation='relu')(z3)\n",
    "c2=Conv2D(16,(3,3),activation='relu')(c1)\n",
    "c3=MaxPooling2D(2,2)(c2) \n",
    "c4=Conv2D(16,(3,3),activation='relu')(c3)\n",
    "c5=MaxPooling2D(2,2)(c4) \n",
    "c6=Conv2D(16,(3,3),activation='relu')(c5)\n",
    "c7=MaxPooling2D(2,2)(c6) \n",
    "c8=Conv2D(64,(1,1),activation='relu')(c7)\n",
    "cc1=Flatten(name='flatten_layer3')(c8)\n",
    "\n",
    "d1=Conv2D(32,(1,1),activation='relu')(z4)\n",
    "d2=Conv2D(32,(3,3),activation='relu')(d1)\n",
    "d3=MaxPooling2D(2,2)(d2) \n",
    "d4=Conv2D(32,(3,3),activation='relu')(d3)\n",
    "d5=MaxPooling2D(2,2)(d4) \n",
    "d6=Conv2D(128,(1,1),activation='relu')(d5)\n",
    "dd1=Flatten(name='flatten_layer4')(d6)\n",
    "\n",
    "e1=Conv2D(64,(1,1),activation='relu')(z5)\n",
    "e2=Conv2D(64,(3,3),activation='relu')(e1)\n",
    "e3=MaxPooling2D(2,2)(e2) \n",
    "e4=Conv2D(64,(2,2),activation='relu')(e3)\n",
    "e5=Conv2D(256,(1,1),activation='relu')(e4)\n",
    "ee1=Flatten(name='flatten_layer5')(e5)\n",
    "\n",
    "fc1=tf.concat([aa1,bb1,cc1,dd1,ee1],axis=1)\n",
    "out1=Dense(units=37, activation='softmax')(fc1)\n",
    "\n",
    "model1 = tf.keras.models.Model(moat1.input, out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d5f9e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      ">=0.1.0 (Current 0.2.1)  1e-14  supported          default: True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended epsilon = 1e-7                                  Recommended epsilon = 1e-14\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AdaBeliefOptimizer' object has no attribute '_set_hyper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#model1.load_weights('model_fulltrain.hdf5')\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m opt \u001b[38;5;241m=\u001b[39m \u001b[43mAdaBeliefOptimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta_1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta_2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.999\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-14\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrectify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m model1\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mopt,\n\u001b[1;32m      5\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mSparseCategoricalAccuracy())\n",
      "File \u001b[0;32m~/Desktop/df/main/tinyMOAT_Ensemble/AdaBelief_tf.py:148\u001b[0m, in \u001b[0;36mAdaBeliefOptimizer.__init__\u001b[0;34m(self, learning_rate, beta_1, beta_2, epsilon, weight_decay, rectify, amsgrad, sma_threshold, total_steps, warmup_proportion, min_lr, name, print_change_log, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28mprint\u001b[39m(Style\u001b[38;5;241m.\u001b[39mRESET_ALL)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_hyper\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m, kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m, learning_rate))\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_hyper(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta_1\u001b[39m\u001b[38;5;124m\"\u001b[39m, beta_1)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_hyper(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta_2\u001b[39m\u001b[38;5;124m\"\u001b[39m, beta_2)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AdaBeliefOptimizer' object has no attribute '_set_hyper'"
     ]
    }
   ],
   "source": [
    "model1.load_weights('model_fulltrain.hdf5')\n",
    "opt = AdaBeliefOptimizer(learning_rate=5e-4, beta_1= 0.9, beta_2=0.999, weight_decay= 1e-4,  epsilon=1e-14, rectify=True)\n",
    "\n",
    "model1.compile(optimizer=opt,\n",
    "              loss='sparse_categorical_crossentropy', metrics = tf.keras.metrics.SparseCategoricalAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc37e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='model_fulltrain2_augment.hdf5', verbose=1, save_best_only=False)\n",
    "csv_logger = CSVLogger('history_augment.log')\n",
    "\n",
    "history = model1.fit_generator(train_generator,\n",
    "                    steps_per_epoch = nb_train_samples // batch_size,      #  한 epoch에 사용한 스텝 수\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=nb_validation_samples // batch_size,  # 한 epoch 종료 시 마다 검증할 때 사용되는 검증 스텝 수\n",
    "                    epochs=200,                                             # 전체 훈련 데이터셋에 대해 학습 반복 횟수\n",
    "                    verbose=1,\n",
    "                    callbacks=[csv_logger, checkpointer])\n",
    "\n",
    "model1.save('model1_trained_augment.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbc71a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e782a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c075d0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
